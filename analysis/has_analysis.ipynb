{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harris' Segmentation Scheme Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:06:18.482629Z",
     "start_time": "2023-04-28T11:06:18.234636Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from joblib import Parallel\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Workaround so we can re-use the metric functions\n",
    "module_path = os.path.abspath(os.path.join(\"../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import trgl.utils.harris_segmentation as has\n",
    "from trgl.utils.harris_segmentation import EntropyCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:06:18.496601Z",
     "start_time": "2023-04-28T11:06:18.483602Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(palette=\"pastel\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "palette = sns.color_palette()\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "params = {\n",
    "    \"legend.title_fontsize\": \"32\",\n",
    "    \"legend.fontsize\": \"24\",\n",
    "    \"axes.labelsize\": \"32\",\n",
    "    \"axes.titlesize\": \"32\",\n",
    "    \"xtick.labelsize\": \"22\",\n",
    "    \"ytick.labelsize\": \"26\",\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "markers = [\"o\", \"+\", \"*\", \"^\", \"v\", \"x\"]\n",
    "hatches = [\"\\\\\", \"+\", \"-\", \"|\", \"x\", \"/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(\"./data/v2/\", \"*.json\"))\n",
    "# For now select random subset for analysis\n",
    "# There's no way we can analyse thousands of files\n",
    "rng = np.random.default_rng()\n",
    "all_files = rng.choice(all_files, size=int(len(all_files) * 0.05))\n",
    "li = []\n",
    "params = []\n",
    "for filename in tqdm(all_files):\n",
    "    split = filename.split(\"-\")\n",
    "    run_id = split[1]\n",
    "    architecture = split[2]\n",
    "    training_dataset = split[3]\n",
    "    net_temporal = split[4].__contains__(\"True\")\n",
    "    loss_temporal = split[5].__contains__(\"True\")\n",
    "    purely_temporal = split[6].__contains__(\"True\")\n",
    "    attention_sender = split[7].__contains__(\"True\")\n",
    "    attention_receiver = split[8].__contains__(\"True\")\n",
    "    eval_dataset = split[9]\n",
    "    # No need to analyse test datasets for language\n",
    "    if eval_dataset not in [\"trg_previous\", \"trg_hard\", \"rg_classic\", \"rg_hard\"]:\n",
    "        continue\n",
    "    params.append(\n",
    "        [\n",
    "            run_id,\n",
    "            training_dataset,\n",
    "            net_temporal,\n",
    "            loss_temporal,\n",
    "            purely_temporal,\n",
    "            attention_sender,\n",
    "            attention_receiver,\n",
    "            eval_dataset,\n",
    "        ]\n",
    "    )\n",
    "    df = pd.read_json(filename, orient=\"index\")\n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is commented out not to bash WandB servers everytime we run analysis\n",
    "# We run this only once and save to pickle\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "# api = wandb.Api(timeout=60)\n",
    "#\n",
    "# runs = api.runs(\"user/TRGL\")\n",
    "# summary_list, config_list, name_list = [], [], []\n",
    "# for run in tqdm(runs):\n",
    "#     summary_list.append(\n",
    "#         run.history(\n",
    "#             samples=400,\n",
    "#         )\n",
    "#     )\n",
    "#\n",
    "#     config_list.append({k: v for k, v in run.config.items()})\n",
    "#\n",
    "#     name_list.append(run.name.split(\"-\")[1])\n",
    "#\n",
    "# runs_full_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"summary\": summary_list,\n",
    "#         \"config\": config_list,\n",
    "#         \"name\": name_list,\n",
    "#     }\n",
    "# )\n",
    "#\n",
    "# runs_full_df.to_pickle(\"./data/v2/runs_full_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:06:18.687430Z",
     "start_time": "2023-04-28T11:06:18.498605Z"
    }
   },
   "outputs": [],
   "source": [
    "runs_full_df = pd.read_pickle(\"./data/v2/runs_full_df.pickle\")\n",
    "runs_full_df = runs_full_df[[\"summary\", \"config\", \"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:06:27.059778Z",
     "start_time": "2023-04-28T11:06:27.049770Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_stats(x: pd.DataFrame):\n",
    "    # Some are empty, we'll drop them later\n",
    "    x = x[x[\"val_acc\"].notnull()]\n",
    "    try:\n",
    "        x[\"val_acc\"].idxmax()\n",
    "    except:\n",
    "        return None\n",
    "    max_acc_index = x[\"val_acc\"].idxmax()\n",
    "    over_75_index = x[\"val_acc\"].ge(0.75).any() and x[\"val_acc\"].ge(0.75).idxmax()\n",
    "    over_85_index = x[\"val_acc\"].ge(0.85).any() and x[\"val_acc\"].ge(0.85).idxmax()\n",
    "    end_acc_index = x[\"epoch\"].idxmax()\n",
    "\n",
    "    stats_dict = {\n",
    "        \"max_acc_epoch\": x[\"epoch\"][max_acc_index],\n",
    "        \"max_acc_value\": x[\"val_acc\"][max_acc_index],\n",
    "        \"over_75_epoch\": x[\"epoch\"][over_75_index] if over_75_index else -1,\n",
    "        \"over_75_value\": x[\"val_acc\"][over_75_index] if over_75_index else -1,\n",
    "        \"over_85_epoch\": x[\"epoch\"][over_85_index] if over_85_index else -1,\n",
    "        \"over_85_value\": x[\"val_acc\"][over_85_index] if over_85_index else -1,\n",
    "        \"end_acc_epoch\": x[\"epoch\"][end_acc_index],\n",
    "        \"end_acc_value\": x[\"val_acc\"][end_acc_index],\n",
    "    }\n",
    "\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:06:30.988354Z",
     "start_time": "2023-04-28T11:06:29.179357Z"
    }
   },
   "outputs": [],
   "source": [
    "runs_full_df[\"summary\"] = runs_full_df[\"summary\"].apply(df_stats).dropna()\n",
    "df_temp = pd.json_normalize(runs_full_df.pop(\"config\"))\n",
    "runs_full_df = runs_full_df.join(df_temp)\n",
    "df_temp = pd.json_normalize(runs_full_df.pop(\"summary\"))\n",
    "runs_full_df = runs_full_df.join(df_temp)\n",
    "runs_full_df = runs_full_df.set_index(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = {\n",
    "    f\"match_{x}\": {\n",
    "        \"run_id\": params[x][0],\n",
    "        \"game_type\": params[x][7],\n",
    "        \"training_dataset\": params[x][1],\n",
    "        \"net_temporal\": params[x][2],\n",
    "        \"loss_temporal\": params[x][3],\n",
    "        \"purely_temporal\": params[x][4],\n",
    "        \"attention_sender\": params[x][5],\n",
    "        \"attention_receiver\": params[x][6],\n",
    "        \"messages\": {},\n",
    "        \"lang\": li[x][\"message\"].to_numpy(),\n",
    "        \"meanings\": li[x][\"target\"].to_numpy(),\n",
    "    }\n",
    "    for x in range(len(li))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm(range(len(matches))):\n",
    "    run_id = matches[f\"match_{idx}\"][\"run_id\"]\n",
    "    matches[f\"match_{idx}\"][\"temporal\"] = runs_full_df.loc[f\"{run_id}\"][\"temporal\"][0]\n",
    "    matches[f\"match_{idx}\"][\"max_epochs\"] = runs_full_df.loc[f\"{run_id}\"][\"max_epochs\"][\n",
    "        0\n",
    "    ]\n",
    "    matches[f\"match_{idx}\"][\"max_length\"] = runs_full_df.loc[f\"{run_id}\"][\"max_length\"][\n",
    "        0\n",
    "    ]\n",
    "    matches[f\"match_{idx}\"][\"num_features\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"num_features\"\n",
    "    ][0]\n",
    "    matches[f\"match_{idx}\"][\"vocab_size\"] = runs_full_df.loc[f\"{run_id}\"][\"vocab_size\"][\n",
    "        0\n",
    "    ]\n",
    "    matches[f\"match_{idx}\"][\"num_objects\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"num_objects\"\n",
    "    ][0]\n",
    "    # Repeat chance was incorrect (flipped) for our first runs, that is why there is inversion later\n",
    "    matches[f\"match_{idx}\"][\"repeat_chance\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"repeat_chance\"\n",
    "    ][0]\n",
    "    matches[f\"match_{idx}\"][\"temporal_loss\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"temporal_loss\"\n",
    "    ][0]\n",
    "    matches[f\"match_{idx}\"][\"length_penalty\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"length_penalty\"\n",
    "    ][0]\n",
    "    matches[f\"match_{idx}\"][\"gs_temperature\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"gs_temperature\"\n",
    "    ][0]\n",
    "    matches[f\"match_{idx}\"][\"num_properties\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"num_properties\"\n",
    "    ][0]\n",
    "    matches[f\"match_{idx}\"][\"num_distractors\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"num_distractors\"\n",
    "    ][0]\n",
    "    matches[f\"match_{idx}\"][\"prev_horizon\"] = runs_full_df.loc[f\"{run_id}\"][\n",
    "        \"prev_horizon\"\n",
    "    ][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for parallel processing\n",
    "# Will compute most HAS related metrics\n",
    "def process_match_has(match: dict):\n",
    "    def get_map_from_len_to_freq_percentage(\n",
    "        lang,\n",
    "        max_len: int,\n",
    "        threshold: float,\n",
    "    ):\n",
    "        len_count = Counter(\n",
    "            map(\n",
    "                len,\n",
    "                itertools.chain.from_iterable(\n",
    "                    EntropyCalculator(lang, threshold=threshold).segments\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        total_value = sum(len_count.values())\n",
    "        return {i + 1: len_count[i + 1] / total_value for i in range(max_len)}\n",
    "\n",
    "    thresholds = (0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2)\n",
    "    entr_calc = EntropyCalculator(match[\"lang\"])\n",
    "    results = {}\n",
    "    for thr in thresholds:\n",
    "        entr_calc.threshold = thr\n",
    "\n",
    "        # Mean number of boundaries\n",
    "        results[f\"thr-{thr}-boundaries\"] = entr_calc.mean_n_boundaries\n",
    "\n",
    "        # Vocab Size\n",
    "        results[f\"thr-{thr}-vocab_size\"] = entr_calc.vocab_size\n",
    "\n",
    "        # Sigurd Style ZLA\n",
    "        results[f\"thr-{thr}-trained_map_len\"] = get_map_from_len_to_freq_percentage(\n",
    "            lang=match[\"lang\"],\n",
    "            max_len=match[\"max_length\"],\n",
    "            threshold=thr,\n",
    "        )\n",
    "\n",
    "        # ZLA\n",
    "        freqs = []\n",
    "        freq_to_lens = defaultdict(list)\n",
    "        for word, freq in Counter(\n",
    "            itertools.chain.from_iterable(entr_calc.segments)\n",
    "        ).most_common():\n",
    "            freqs.append(freq)\n",
    "            freq_to_lens[freq].append(len(word))\n",
    "        results[f\"thr-{thr}-zla\"] = [np.mean(freq_to_lens[freq]) for freq in freqs]\n",
    "\n",
    "        # Zipf\n",
    "        freqs = [\n",
    "            x[1]\n",
    "            for x in Counter(\n",
    "                itertools.chain.from_iterable(\n",
    "                    EntropyCalculator(match[\"lang\"], threshold=thr).segments\n",
    "                )\n",
    "            ).most_common()\n",
    "        ]\n",
    "        results[f\"thr-{thr}-zipf\"] = freqs\n",
    "\n",
    "        # Topographic Similarities\n",
    "        results[f\"thr-{thr}-thr_to_topsims\"] = has.compute_topsim(\n",
    "            entr_calc.hashed_segments, match[\"meanings\"]\n",
    "        )\n",
    "        results[f\"thr-{thr}-thr_to_random_seg_topsims\"] = has.compute_topsim(\n",
    "            entr_calc.hashed_random_segments, match[\"meanings\"]\n",
    "        )\n",
    "\n",
    "    # Entropies\n",
    "    results[f\"entropies\"] = [\n",
    "        v\n",
    "        for _, v in sorted(EntropyCalculator(match[\"lang\"]).conditional_entropy.items())\n",
    "    ]\n",
    "\n",
    "    # Plain Top Sim\n",
    "    results[f\"plain_topsim\"] = has.compute_topsim(match[\"lang\"], match[\"meanings\"])\n",
    "\n",
    "    del entr_calc\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.__stdout__.write(\"foo\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "results = Parallel(n_jobs=os.cpu_count(), verbose=10)(\n",
    "    (process_match_has, (matches[match],), {}) for match in matches\n",
    ")\n",
    "\n",
    "for x in range(len(li)):\n",
    "    matches[f\"match_{x}\"].update(results[x])\n",
    "\n",
    "finish_time = time.perf_counter()\n",
    "print(f\"Computing strategy stats finished in {finish_time-start_time} seconds\")\n",
    "sys.__stdout__.write(\n",
    "    f\"Computing strategy stats finished in {finish_time-start_time} seconds\\n\"\n",
    ")\n",
    "del results\n",
    "\n",
    "# Save to pickle as computation takes a very long time.\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.T\n",
    "matches_df.to_pickle(\"./data/v2/matches_has.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:22:34.618948Z",
     "start_time": "2023-04-28T11:22:33.321929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read from pickle - should be no difference and progress saved!\n",
    "matches_df = pd.read_pickle(\"./data/v2/matches_has.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:22:35.346929Z",
     "start_time": "2023-04-28T11:22:35.332940Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "matches_df = matches_df.sort_values(\n",
    "    [\"net_temporal\", \"loss_temporal\", \"training_dataset\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot N Hypothetical Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:33:49.101641Z",
     "start_time": "2023-04-28T11:33:47.976659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig1 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == True)\n",
    "    & (matches_df[\"attention_receiver\"] == True)\n",
    "].filter(like=\"boundaries\")\n",
    "matches_df_fig1 = matches_df_fig1.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig1 = matches_df_fig1.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig1 = matches_df_fig1.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig1 = matches_df_fig1.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig1 = matches_df_fig1.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"boundary\",\n",
    "    value_name=\"number\",\n",
    ")\n",
    "matches_df_fig1[\"boundary\"] = matches_df_fig1[\"boundary\"].apply(\n",
    "    lambda x: x.split(\"-\")[1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.boxplot(\n",
    "    data=matches_df_fig1,\n",
    "    y=\"number\",\n",
    "    x=\"boundary\",\n",
    "    hue=matches_df_fig1[[\"net_temporal\", \"loss_temporal\", \"training_dataset\"]].apply(\n",
    "        tuple, axis=1\n",
    "    ),\n",
    "    ax=ax,\n",
    ").set(\n",
    "    xlabel=\"Threshold Value\",\n",
    "    ylabel=\"Mean N of HypoBoundaries\",\n",
    ")\n",
    "labels = [\n",
    "    \"Non-Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Non-Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Non-Temporal Net trained on RGs\",\n",
    "    \"Non-Temporal Net trained on TRGs\",\n",
    "    \"Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Temporal Net trained on RGs\",\n",
    "    \"Temporal Net trained on TRGs\",\n",
    "]\n",
    "hatches = [\"//\", \"..\", \"xx\", \"OO\", \"\\\\\\\\\", \"**\", \"||\", \"--\"]\n",
    "patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "# the number of patches should be evenly divisible by the number of hatches\n",
    "h = hatches * (len(patches) // len(hatches))\n",
    "# iterate through the patches for each subplot\n",
    "for patch, hatch in zip(patches, h):\n",
    "    patch.set_hatch(hatch)\n",
    "    fc = patch.get_facecolor()\n",
    "    patch.set_edgecolor(fc)\n",
    "    patch.set_facecolor(\"none\")\n",
    "h, _ = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    labels,\n",
    "    title=\"Network Type\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.5, 1.5),\n",
    "    loc=\"upper center\",\n",
    ")\n",
    "for lp, hatch in zip(l.get_patches(), hatches):\n",
    "    lp.set_hatch(hatch)\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor(\"none\")\n",
    "fig.savefig(\"n_hypothetical_boundaries_at.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:33:51.732586Z",
     "start_time": "2023-04-28T11:33:50.125641Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig1 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == False)\n",
    "    & (matches_df[\"attention_receiver\"] == False)\n",
    "].filter(like=\"boundaries\")\n",
    "matches_df_fig1 = matches_df_fig1.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig1 = matches_df_fig1.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig1 = matches_df_fig1.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig1 = matches_df_fig1.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig1 = matches_df_fig1.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"boundary\",\n",
    "    value_name=\"number\",\n",
    ")\n",
    "matches_df_fig1[\"boundary\"] = matches_df_fig1[\"boundary\"].apply(\n",
    "    lambda x: x.split(\"-\")[1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.boxplot(\n",
    "    data=matches_df_fig1,\n",
    "    y=\"number\",\n",
    "    x=\"boundary\",\n",
    "    hue=matches_df_fig1[[\"net_temporal\", \"loss_temporal\", \"training_dataset\"]].apply(\n",
    "        tuple, axis=1\n",
    "    ),\n",
    "    ax=ax,\n",
    ").set(\n",
    "    xlabel=\"Threshold Value\",\n",
    "    ylabel=\"Mean N of HypoBoundaries\",\n",
    ")\n",
    "labels = [\n",
    "    \"Non-Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Non-Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Non-Temporal Net trained on RGs\",\n",
    "    \"Non-Temporal Net trained on TRGs\",\n",
    "    \"Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Temporal Net trained on RGs\",\n",
    "    \"Temporal Net trained on TRGs\",\n",
    "]\n",
    "hatches = [\"//\", \"..\", \"xx\", \"OO\", \"\\\\\\\\\", \"**\", \"||\", \"--\"]\n",
    "patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "# the number of patches should be evenly divisible by the number of hatches\n",
    "h = hatches * (len(patches) // len(hatches))\n",
    "# iterate through the patches for each subplot\n",
    "for patch, hatch in zip(patches, h):\n",
    "    patch.set_hatch(hatch)\n",
    "    fc = patch.get_facecolor()\n",
    "    patch.set_edgecolor(fc)\n",
    "    patch.set_facecolor(\"none\")\n",
    "h, _ = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    labels,\n",
    "    title=\"Network Type\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.5, 1.5),\n",
    "    loc=\"upper center\",\n",
    ")\n",
    "for lp, hatch in zip(l.get_patches(), hatches):\n",
    "    lp.set_hatch(hatch)\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor(\"none\")\n",
    "fig.savefig(\"n_hypothetical_boundaries_noat.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:35:00.814137Z",
     "start_time": "2023-04-28T11:34:59.766132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig2 = (\n",
    "    matches_df.loc[\n",
    "        (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "        & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "        & (matches_df[\"attention_sender\"] == True)\n",
    "        & (matches_df[\"attention_receiver\"] == True)\n",
    "    ]\n",
    "    .filter(like=\"vocab_size\")\n",
    "    .filter(like=\"thr\")\n",
    ")\n",
    "matches_df_fig2 = matches_df_fig2.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig2 = matches_df_fig2.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig2 = matches_df_fig2.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig2 = matches_df_fig2.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig2 = matches_df_fig2.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"boundary\",\n",
    "    value_name=\"number\",\n",
    ")\n",
    "matches_df_fig2[\"boundary\"] = matches_df_fig2[\"boundary\"].apply(\n",
    "    lambda x: x.split(\"-\")[1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.boxplot(\n",
    "    data=matches_df_fig2,\n",
    "    y=\"number\",\n",
    "    x=\"boundary\",\n",
    "    hue=matches_df_fig2[[\"net_temporal\", \"loss_temporal\", \"training_dataset\"]].apply(\n",
    "        tuple, axis=1\n",
    "    ),\n",
    "    ax=ax,\n",
    ").set(\n",
    "    xlabel=\"Threshold Value\",\n",
    "    ylabel=\"Vocab Size\",\n",
    ")\n",
    "labels = [\n",
    "    \"Non-Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Non-Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Non-Temporal Net trained on RGs\",\n",
    "    \"Non-Temporal Net trained on TRGs\",\n",
    "    \"Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Temporal Net trained on RGs\",\n",
    "    \"Temporal Net trained on TRGs\",\n",
    "]\n",
    "hatches = [\"//\", \"..\", \"xx\", \"OO\", \"\\\\\\\\\", \"**\", \"||\", \"--\"]\n",
    "patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "# the number of patches should be evenly divisible by the number of hatches\n",
    "h = hatches * (len(patches) // len(hatches))\n",
    "# iterate through the patches for each subplot\n",
    "for patch, hatch in zip(patches, h):\n",
    "    patch.set_hatch(hatch)\n",
    "    fc = patch.get_facecolor()\n",
    "    patch.set_edgecolor(fc)\n",
    "    patch.set_facecolor(\"none\")\n",
    "h, _ = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    labels,\n",
    "    title=\"Network Type\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.5, 1.5),\n",
    "    loc=\"upper center\",\n",
    ")\n",
    "for lp, hatch in zip(l.get_patches(), hatches):\n",
    "    lp.set_hatch(hatch)\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor(\"none\")\n",
    "fig.savefig(\"vocab_size_at.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:35:03.240159Z",
     "start_time": "2023-04-28T11:35:01.548130Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig2 = (\n",
    "    matches_df.loc[\n",
    "        (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "        & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "        & (matches_df[\"attention_sender\"] == False)\n",
    "        & (matches_df[\"attention_receiver\"] == False)\n",
    "    ]\n",
    "    .filter(like=\"vocab_size\")\n",
    "    .filter(like=\"thr\")\n",
    ")\n",
    "matches_df_fig2 = matches_df_fig2.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig2 = matches_df_fig2.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig2 = matches_df_fig2.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig2 = matches_df_fig2.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig2 = matches_df_fig2.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"boundary\",\n",
    "    value_name=\"number\",\n",
    ")\n",
    "matches_df_fig2[\"boundary\"] = matches_df_fig2[\"boundary\"].apply(\n",
    "    lambda x: x.split(\"-\")[1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.boxplot(\n",
    "    data=matches_df_fig2,\n",
    "    y=\"number\",\n",
    "    x=\"boundary\",\n",
    "    hue=matches_df_fig2[[\"net_temporal\", \"loss_temporal\", \"training_dataset\"]].apply(\n",
    "        tuple, axis=1\n",
    "    ),\n",
    "    ax=ax,\n",
    ").set(\n",
    "    xlabel=\"Threshold Value\",\n",
    "    ylabel=\"Vocab Size\",\n",
    ")\n",
    "labels = [\n",
    "    \"Non-Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Non-Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Non-Temporal Net trained on RGs\",\n",
    "    \"Non-Temporal Net trained on TRGs\",\n",
    "    \"Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Temporal Net trained on RGs\",\n",
    "    \"Temporal Net trained on TRGs\",\n",
    "]\n",
    "hatches = [\"//\", \"..\", \"xx\", \"OO\", \"\\\\\\\\\", \"**\", \"||\", \"--\"]\n",
    "patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "# the number of patches should be evenly divisible by the number of hatches\n",
    "h = hatches * (len(patches) // len(hatches))\n",
    "# iterate through the patches for each subplot\n",
    "for patch, hatch in zip(patches, h):\n",
    "    patch.set_hatch(hatch)\n",
    "    fc = patch.get_facecolor()\n",
    "    patch.set_edgecolor(fc)\n",
    "    patch.set_facecolor(\"none\")\n",
    "h, _ = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    labels,\n",
    "    title=\"Network Type\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.5, 1.5),\n",
    "    loc=\"upper center\",\n",
    ")\n",
    "for lp, hatch in zip(l.get_patches(), hatches):\n",
    "    lp.set_hatch(hatch)\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor(\"none\")\n",
    "fig.savefig(\"vocab_size_noat.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ZLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:35:48.965418Z",
     "start_time": "2023-04-28T11:35:44.940419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig3 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == True)\n",
    "    & (matches_df[\"attention_receiver\"] == True)\n",
    "].filter(like=\"zla\")\n",
    "matches_df_fig3 = matches_df_fig3.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig3 = matches_df_fig3.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig3 = matches_df_fig3.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig3 = matches_df_fig3.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig3 = matches_df_fig3.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"threshold\",\n",
    "    value_name=\"zla\",\n",
    ")\n",
    "matches_df_fig3[\"threshold\"] = matches_df_fig3[\"threshold\"].apply(\n",
    "    lambda x: x.split(\"-\")[1]\n",
    ")\n",
    "\n",
    "thresholds = (0, 0.5, 1.5, 2)\n",
    "max_rank = 200\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(32, 20), nrows=2, ncols=2)\n",
    "axes = axes.reshape(-1)\n",
    "for horizon in range(1, 5):\n",
    "    for thr in thresholds:\n",
    "        data_list = matches_df_fig3[matches_df_fig3[\"threshold\"] == f\"{thr}\"][\"zla\"]\n",
    "        y = np.array(\n",
    "            [np.mean([e for e in x if e is not None]) for x in data_list][:max_rank]\n",
    "        )\n",
    "        y_sem = np.array(\n",
    "            [\n",
    "                has.standard_error_of_mean([e for e in x if e is not None])\n",
    "                for x in data_list\n",
    "            ][:max_rank]\n",
    "        )\n",
    "        x = np.arange(np.size(y)) + 1\n",
    "        sns.lineplot(x=x, y=y, label=\"network type, threshold\", ax=axes[horizon - 1])\n",
    "        axes[horizon - 1].fill_between(\n",
    "            x,\n",
    "            y - y_sem,\n",
    "            y + y_sem,\n",
    "            color=ax.get_lines()[-1].get_color(),\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    axes[horizon - 1].legend(bbox_to_anchor=(0.5, -0.25), loc=\"upper center\")\n",
    "    axes[horizon - 1].set_xlabel(\"Frequency Rank\")\n",
    "    axes[horizon - 1].set_ylabel(\"Hypo-segment Length\")\n",
    "    axes[horizon - 1].set_xscale(\"log\")\n",
    "    axes[horizon - 1].set_yscale(\"log\")\n",
    "fig.savefig(\"zla_attval_at.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:35:57.628200Z",
     "start_time": "2023-04-28T11:35:53.280234Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig3 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == False)\n",
    "    & (matches_df[\"attention_receiver\"] == False)\n",
    "].filter(like=\"zla\")\n",
    "matches_df_fig3 = matches_df_fig3.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig3 = matches_df_fig3.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig3 = matches_df_fig3.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig3 = matches_df_fig3.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig3 = matches_df_fig3.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"threshold\",\n",
    "    value_name=\"zla\",\n",
    ")\n",
    "matches_df_fig3[\"threshold\"] = matches_df_fig3[\"threshold\"].apply(\n",
    "    lambda x: x.split(\"-\")[1]\n",
    ")\n",
    "\n",
    "thresholds = (0, 0.5, 1.5, 2)\n",
    "max_rank = 200\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(32, 20), nrows=2, ncols=2)\n",
    "axes = axes.reshape(-1)\n",
    "for horizon in range(1, 5):\n",
    "    for thr in thresholds:\n",
    "        data_list = matches_df_fig3[matches_df_fig3[\"threshold\"] == f\"{thr}\"][\"zla\"]\n",
    "        y = np.array(\n",
    "            [np.mean([e for e in x if e is not None]) for x in data_list][:max_rank]\n",
    "        )\n",
    "        y_sem = np.array(\n",
    "            [\n",
    "                has.standard_error_of_mean([e for e in x if e is not None])\n",
    "                for x in data_list\n",
    "            ][:max_rank]\n",
    "        )\n",
    "        x = np.arange(np.size(y)) + 1\n",
    "        sns.lineplot(x=x, y=y, label=\"network type, threshold\", ax=axes[horizon - 1])\n",
    "        axes[horizon - 1].fill_between(\n",
    "            x,\n",
    "            y - y_sem,\n",
    "            y + y_sem,\n",
    "            color=ax.get_lines()[-1].get_color(),\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    axes[horizon - 1].legend(bbox_to_anchor=(0.5, -0.25), loc=\"upper center\")\n",
    "    axes[horizon - 1].set_xlabel(\"Frequency Rank\")\n",
    "    axes[horizon - 1].set_ylabel(\"Hypo-segment Length\")\n",
    "    axes[horizon - 1].set_xscale(\"log\")\n",
    "    axes[horizon - 1].set_yscale(\"log\")\n",
    "fig.savefig(\"zla_attval_noat.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Conditional Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:23:41.741644Z",
     "start_time": "2023-04-28T11:23:39.702381Z"
    }
   },
   "outputs": [],
   "source": [
    "matches_df_fig4 = pd.DataFrame(\n",
    "    [\n",
    "        pd.Series(x)\n",
    "        for x in matches_df.loc(\n",
    "            (matches_df[\"attention_sender\"] == True)\n",
    "            & (matches_df[\"attention_receiver\"] == True)\n",
    "        ).entropies\n",
    "    ]\n",
    ")\n",
    "matches_df_fig4.columns = [\"entropy_{}\".format(x + 1) for x in matches_df_fig4.columns]\n",
    "matches_df_fig4 = matches_df_fig4.set_index(matches_df.index[: len(matches_df_fig4)])\n",
    "matches_df_fig4 = matches_df_fig4.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig4 = matches_df_fig4.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig4 = matches_df_fig4.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig4 = matches_df_fig4.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "\n",
    "matches_df_fig4 = matches_df_fig4.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"position\",\n",
    "    value_name=\"value\",\n",
    ")\n",
    "matches_df_fig4[\"position\"] = matches_df_fig4[\"position\"].apply(\n",
    "    lambda x: x.split(\"_\")[1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.lineplot(\n",
    "    data=matches_df_fig4,\n",
    "    x=\"position\",\n",
    "    y=\"value\",\n",
    "    hue=matches_df_fig1[[\"net_temporal\", \"loss_temporal\", \"training_dataset\"]].apply(\n",
    "        tuple, axis=1\n",
    "    ),\n",
    ").set(\n",
    "    xlabel=\"Character Position\",\n",
    "    ylabel=\"Conditional Entropy\",\n",
    ")\n",
    "labels = [\n",
    "    \"Non-Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Non-Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Non-Temporal Net trained on RGs\",\n",
    "    \"Non-Temporal Net trained on TRGs\",\n",
    "    \"Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Temporal Net trained on RGs\",\n",
    "    \"Temporal Net trained on TRGs\",\n",
    "]\n",
    "h, _ = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    labels,\n",
    "    title=\"Network Type\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.5, 1.5),\n",
    "    loc=\"upper center\",\n",
    ")\n",
    "fig.savefig(\"conditional_entropy_at.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "matches_df_fig4 = pd.DataFrame(\n",
    "    [\n",
    "        pd.Series(x)\n",
    "        for x in matches_df.loc(\n",
    "            (matches_df[\"attention_sender\"] == False)\n",
    "            & (matches_df[\"attention_receiver\"] == False)\n",
    "        ).entropies\n",
    "    ]\n",
    ")\n",
    "matches_df_fig4.columns = [\"entropy_{}\".format(x + 1) for x in matches_df_fig4.columns]\n",
    "matches_df_fig4 = matches_df_fig4.set_index(matches_df.index[: len(matches_df_fig4)])\n",
    "matches_df_fig4 = matches_df_fig4.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig4 = matches_df_fig4.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig4 = matches_df_fig4.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig4 = matches_df_fig4.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "\n",
    "matches_df_fig4 = matches_df_fig4.melt(\n",
    "    id_vars=[\"game_type\", \"net_temporal\", \"loss_temporal\", \"training_dataset\"],\n",
    "    var_name=\"position\",\n",
    "    value_name=\"value\",\n",
    ")\n",
    "matches_df_fig4[\"position\"] = matches_df_fig4[\"position\"].apply(\n",
    "    lambda x: x.split(\"_\")[1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.lineplot(\n",
    "    data=matches_df_fig4,\n",
    "    x=\"position\",\n",
    "    y=\"value\",\n",
    "    hue=matches_df_fig1[[\"net_temporal\", \"loss_temporal\", \"training_dataset\"]].apply(\n",
    "        tuple, axis=1\n",
    "    ),\n",
    ").set(\n",
    "    xlabel=\"Character Position\",\n",
    "    ylabel=\"Conditional Entropy\",\n",
    ")\n",
    "labels = [\n",
    "    \"Non-Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Non-Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Non-Temporal Net trained on RGs\",\n",
    "    \"Non-Temporal Net trained on TRGs\",\n",
    "    \"Temporal Net w/o t-loss trained on RGs\",\n",
    "    \"Temporal Net w/o t-loss trained on TRGs\",\n",
    "    \"Temporal Net trained on RGs\",\n",
    "    \"Temporal Net trained on TRGs\",\n",
    "]\n",
    "h, _ = ax.get_legend_handles_labels()\n",
    "l = ax.legend(\n",
    "    h,\n",
    "    labels,\n",
    "    title=\"Network Type\",\n",
    "    ncols=2,\n",
    "    bbox_to_anchor=(0.5, 1.5),\n",
    "    loc=\"upper center\",\n",
    ")\n",
    "fig.savefig(\"conditional_entropy_noat.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sample Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:14:25.437237Z",
     "start_time": "2023-03-31T14:14:24.322236Z"
    }
   },
   "outputs": [],
   "source": [
    "random_seed: int = 0\n",
    "\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "threshold = 0\n",
    "\n",
    "entr = EntropyCalculator(\n",
    "    matches_df[\"lang\"].sample().to_numpy()[0],\n",
    "    threshold=threshold,\n",
    ")\n",
    "utter_id: int = rng.choice(range(len(entr.data)))\n",
    "utter = entr.data[utter_id]\n",
    "boundaries = entr.boundaries[utter_id]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    1,\n",
    "    1,\n",
    "    sharex=\"all\",\n",
    "    sharey=\"all\",\n",
    "    figsize=(14, 8),\n",
    ")\n",
    "for i, boundary in enumerate(sorted(boundaries)):\n",
    "    ax.plot([boundary - 0.5] * 2, [-0.3, 4], linestyle=\"--\", color=\"black\")\n",
    "for i, boundary in enumerate(sorted(boundaries)):\n",
    "    bottom = boundary - 2\n",
    "    while not (\n",
    "        entr.branching_entropy[utter[bottom:boundary]]\n",
    "        - entr.branching_entropy[utter[bottom : boundary - 1]]\n",
    "        > entr.threshold\n",
    "    ):\n",
    "        assert bottom >= 0\n",
    "        bottom -= 1\n",
    "    anno_data = [utter[bottom : up + 1] for up in range(bottom, boundary)]\n",
    "    x_plot_data = [x + 0.5 for x in range(bottom, boundary)]\n",
    "    y_plot_data = [entr.branching_entropy[x] for x in anno_data]\n",
    "    ax.plot(\n",
    "        x_plot_data,\n",
    "        y_plot_data,\n",
    "        marker={\n",
    "            0: \"v\",\n",
    "            1: \"^\",\n",
    "            2: \"<\",\n",
    "            3: \">\",\n",
    "        }[i % 4],\n",
    "        label=(f\"transition for boundary {i+1}\"),\n",
    "        linewidth=3,\n",
    "    )\n",
    "    for idx in reversed(range(len(anno_data)) if len(anno_data) < 3 else (0, -2, -1)):\n",
    "        anno = anno_data[idx]\n",
    "        x, y = x_plot_data[idx], y_plot_data[idx]\n",
    "        anno_str = (\n",
    "            \"$h(\"\n",
    "            + (\n",
    "                \",\".join(map(str, anno))\n",
    "                if len(anno) < 5\n",
    "                else f\"{anno[0]},\\\\ldots,{anno[-2]},{anno[-1]}\"\n",
    "            )\n",
    "            + \")$\"\n",
    "        )\n",
    "        ax.annotate(\n",
    "            anno_str,\n",
    "            (x, y),\n",
    "            (x + 0.25, y + 0.25),\n",
    "            bbox=dict(\n",
    "                boxstyle=\"round\",\n",
    "                facecolor=\"white\",\n",
    "                alpha=0.5,\n",
    "            ),\n",
    "            arrowprops=dict(\n",
    "                arrowstyle=\"->\",\n",
    "                connectionstyle=\"arc3\",\n",
    "            ),\n",
    "        )\n",
    "    prev_boundary = boundary\n",
    "ax.legend(bbox_to_anchor=(0.5, 1.25), loc=\"upper center\", ncol=2)\n",
    "ax.set_xticks(list(range(len(utter))))\n",
    "ax.set_xticklabels([str(u) for u in utter])\n",
    "ax.set_xlabel(\"message\")\n",
    "ax.set_ylabel(\"$h$\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f\"branching_entropy_sample_thr{threshold}_seed{random_seed}.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot TopSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:36:34.483312Z",
     "start_time": "2023-04-28T11:36:34.029015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig6_t = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == True)\n",
    "    & (matches_df[\"attention_receiver\"] == True)\n",
    "].filter(like=\"plain_topsim\")\n",
    "\n",
    "matches_df_fig6 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == True)\n",
    "    & (matches_df[\"attention_receiver\"] == True)\n",
    "].filter(like=\"to_topsims\")\n",
    "matches_df_fig6 = matches_df_fig6.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig6 = matches_df_fig6.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig6 = matches_df_fig6.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig6 = matches_df_fig6.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig6 = pd.concat([matches_df_fig6, matches_df_fig6_t], axis=1)\n",
    "\n",
    "\n",
    "thresholds = (0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2)\n",
    "xlabel: str = \"$threshold$\"\n",
    "ylabel: str = \"TopSim\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "for i in range(1):\n",
    "    data_lists = [matches_df_fig6[\"plain_topsim\"].to_numpy()] + [\n",
    "        matches_df_fig6[f\"thr-{thr}-thr_to_topsims\"].to_numpy() for thr in thresholds\n",
    "    ]\n",
    "    x = np.array([-0.25] + list(thresholds))\n",
    "    y = np.array([np.mean(d) for d in data_lists])\n",
    "    y_sem = np.array([np.std(d, ddof=1) / np.sqrt(np.size(d)) for d in data_lists])\n",
    "    sns.lineplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        label=\"a\",\n",
    "        marker=markers[i % len(markers)],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        y - y_sem,\n",
    "        y + y_sem,\n",
    "        color=ax.get_lines()[-1].get_color(),\n",
    "        alpha=0.3,\n",
    "    )\n",
    "ax.legend()\n",
    "ax.set_xlabel(xlabel)\n",
    "ax.set_ylabel(ylabel)\n",
    "ax.set_xticks((-0.25,) + tuple(thresholds))\n",
    "ax.set_xticklabels((\"$-\\\\infty$\",) + tuple(thresholds))\n",
    "fig.savefig(\"topsim_at.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:36:39.510603Z",
     "start_time": "2023-04-28T11:36:38.883600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "matches_df_fig6_t = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == False)\n",
    "    & (matches_df[\"attention_receiver\"] == False)\n",
    "].filter(like=\"plain_topsim\")\n",
    "\n",
    "matches_df_fig6 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == False)\n",
    "    & (matches_df[\"attention_receiver\"] == False)\n",
    "].filter(like=\"to_topsims\")\n",
    "matches_df_fig6 = matches_df_fig6.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig6 = matches_df_fig6.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig6 = matches_df_fig6.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig6 = matches_df_fig6.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "matches_df_fig6 = pd.concat([matches_df_fig6, matches_df_fig6_t], axis=1)\n",
    "\n",
    "\n",
    "thresholds = (0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2)\n",
    "xlabel: str = \"$threshold$\"\n",
    "ylabel: str = \"TopSim\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "for i in range(1):\n",
    "    data_lists = [matches_df_fig6[\"plain_topsim\"].to_numpy()] + [\n",
    "        matches_df_fig6[f\"thr-{thr}-thr_to_topsims\"].to_numpy() for thr in thresholds\n",
    "    ]\n",
    "    x = np.array([-0.25] + list(thresholds))\n",
    "    y = np.array([np.mean(d) for d in data_lists])\n",
    "    y_sem = np.array([np.std(d, ddof=1) / np.sqrt(np.size(d)) for d in data_lists])\n",
    "    sns.lineplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        label=\"a\",\n",
    "        marker=markers[i % len(markers)],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        y - y_sem,\n",
    "        y + y_sem,\n",
    "        color=ax.get_lines()[-1].get_color(),\n",
    "        alpha=0.3,\n",
    "    )\n",
    "ax.legend()\n",
    "ax.set_xlabel(xlabel)\n",
    "ax.set_ylabel(ylabel)\n",
    "ax.set_xticks((-0.25,) + tuple(thresholds))\n",
    "ax.set_xticklabels((\"$-\\\\infty$\",) + tuple(thresholds))\n",
    "fig.savefig(\"topsim_noat.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Topsim Compare to Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:37:08.763438Z",
     "start_time": "2023-04-28T11:37:08.071437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "# First we create the random seg topsims\n",
    "matches_df_fig7_1 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == True)\n",
    "    & (matches_df[\"attention_receiver\"] == True)\n",
    "].filter(like=\"seg_topsims\")\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "\n",
    "# Then threshold topsims\n",
    "matches_df_fig7_2 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "].filter(like=\"to_topsims\")\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "\n",
    "thresholds = (0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2)\n",
    "xlabel: str = \"$threshold$\"\n",
    "ylabel: str = \"TopSim\"\n",
    "attval_format: str = \"$(n_{{att}},n_{{val}})={}$\"\n",
    "verbose: bool = True\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "data_lists = [\n",
    "    matches_df_fig7_2[f\"thr-{thr}-thr_to_topsims\"].to_numpy() for thr in thresholds\n",
    "]\n",
    "random_seg_data_lists = [\n",
    "    matches_df_fig7_1[f\"thr-{thr}-thr_to_random_seg_topsims\"].to_numpy()\n",
    "    for thr in thresholds\n",
    "]\n",
    "x = np.array(list(thresholds))\n",
    "y = np.array([np.mean(d) for d in data_lists])\n",
    "y_sem = np.array([np.std(d, ddof=1) / np.sqrt(np.size(d)) for d in data_lists])\n",
    "y_random = np.array([np.mean(d) for d in random_seg_data_lists])\n",
    "y_random_sem = np.array(\n",
    "    [np.std(d, ddof=1) / np.sqrt(np.size(d)) for d in random_seg_data_lists]\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    y,\n",
    "    label=\"a\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    x,\n",
    "    y - y_sem,\n",
    "    y + y_sem,\n",
    "    color=ax.get_lines()[-1].get_color(),\n",
    "    alpha=0.3,\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    y_random,\n",
    "    label=\"a\" + \" (random boundary)\",\n",
    "    marker=\"D\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    x,\n",
    "    y_random - y_random_sem,\n",
    "    y_random + y_random_sem,\n",
    "    color=ax.get_lines()[-1].get_color(),\n",
    "    alpha=0.3,\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(xlabel)\n",
    "ax.set_ylabel(ylabel)\n",
    "fig.savefig(\"topsim_attval{attval}_vs_random_baseline_at.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T11:37:13.620776Z",
     "start_time": "2023-04-28T11:37:13.144100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess dataframe to fit our graph nicely\n",
    "# First we create the random seg topsims\n",
    "matches_df_fig7_1 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "    & (matches_df[\"attention_sender\"] == False)\n",
    "    & (matches_df[\"attention_receiver\"] == False)\n",
    "].filter(like=\"seg_topsims\")\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig7_1 = matches_df_fig7_1.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "\n",
    "# Then threshold topsims\n",
    "matches_df_fig7_2 = matches_df.loc[\n",
    "    (matches_df[\"game_type\"] != \"analysis_always_same\")\n",
    "    & (matches_df[\"game_type\"] != \"analysis_never_same\")\n",
    "].filter(like=\"to_topsims\")\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(game_type=matches_df[\"game_type\"])\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(loss_temporal=matches_df[\"loss_temporal\"])\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(net_temporal=matches_df[\"net_temporal\"])\n",
    "matches_df_fig7_2 = matches_df_fig7_2.assign(\n",
    "    training_dataset=matches_df[\"training_dataset\"]\n",
    ")\n",
    "\n",
    "thresholds = (0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2)\n",
    "xlabel: str = \"$threshold$\"\n",
    "ylabel: str = \"TopSim\"\n",
    "attval_format: str = \"$(n_{{att}},n_{{val}})={}$\"\n",
    "verbose: bool = True\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "data_lists = [\n",
    "    matches_df_fig7_2[f\"thr-{thr}-thr_to_topsims\"].to_numpy() for thr in thresholds\n",
    "]\n",
    "random_seg_data_lists = [\n",
    "    matches_df_fig7_1[f\"thr-{thr}-thr_to_random_seg_topsims\"].to_numpy()\n",
    "    for thr in thresholds\n",
    "]\n",
    "x = np.array(list(thresholds))\n",
    "y = np.array([np.mean(d) for d in data_lists])\n",
    "y_sem = np.array([np.std(d, ddof=1) / np.sqrt(np.size(d)) for d in data_lists])\n",
    "y_random = np.array([np.mean(d) for d in random_seg_data_lists])\n",
    "y_random_sem = np.array(\n",
    "    [np.std(d, ddof=1) / np.sqrt(np.size(d)) for d in random_seg_data_lists]\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    y,\n",
    "    label=\"a\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    x,\n",
    "    y - y_sem,\n",
    "    y + y_sem,\n",
    "    color=ax.get_lines()[-1].get_color(),\n",
    "    alpha=0.3,\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    y_random,\n",
    "    label=\"a\" + \" (random boundary)\",\n",
    "    marker=\"D\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    x,\n",
    "    y_random - y_random_sem,\n",
    "    y_random + y_random_sem,\n",
    "    color=ax.get_lines()[-1].get_color(),\n",
    "    alpha=0.3,\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(xlabel)\n",
    "ax.set_ylabel(ylabel)\n",
    "fig.savefig(\"topsim_attval{attval}_vs_random_baseline_noat.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
